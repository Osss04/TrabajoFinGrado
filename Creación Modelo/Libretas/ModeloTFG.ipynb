{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SWaT: Creación del modelo</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introducción</h2>\n",
    "\n",
    "En esta libreta creamos el modelo correspondiente con los datos obtenidos de la libreta **SWaT: Preparando los datos**. Explicaremos los pasos necesarios para crear un modelo de detección de anomalías para el DataSet SWaT una vez ya preprocesado y justificaremos las decisiones que hemos tomado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Cargando los datos ya preparados</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección cargamos los .csv que genereamos en la libreta anterior para poder trabajar con los datos del dataset SWaT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_csv(\"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos si los datos se han cargado de manera correcta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT101</th>\n",
       "      <th>LIT101</th>\n",
       "      <th>FIT201</th>\n",
       "      <th>DPIT301</th>\n",
       "      <th>LIT301</th>\n",
       "      <th>LIT401</th>\n",
       "      <th>FIT601</th>\n",
       "      <th>std_FIT101_120</th>\n",
       "      <th>std_LIT101_120</th>\n",
       "      <th>std_FIT201_120</th>\n",
       "      <th>...</th>\n",
       "      <th>MV303_1</th>\n",
       "      <th>MV303_2</th>\n",
       "      <th>MV304_0</th>\n",
       "      <th>MV304_1</th>\n",
       "      <th>MV304_2</th>\n",
       "      <th>P302_1</th>\n",
       "      <th>P302_2</th>\n",
       "      <th>P602_1</th>\n",
       "      <th>P602_2</th>\n",
       "      <th>Normal/Attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894061</td>\n",
       "      <td>0.145253</td>\n",
       "      <td>0.980632</td>\n",
       "      <td>0.966672</td>\n",
       "      <td>0.817110</td>\n",
       "      <td>0.964381</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.888111</td>\n",
       "      <td>0.144064</td>\n",
       "      <td>0.981095</td>\n",
       "      <td>0.965737</td>\n",
       "      <td>0.818515</td>\n",
       "      <td>0.966238</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.884144</td>\n",
       "      <td>0.144540</td>\n",
       "      <td>0.981095</td>\n",
       "      <td>0.964336</td>\n",
       "      <td>0.822205</td>\n",
       "      <td>0.965394</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.883677</td>\n",
       "      <td>0.143826</td>\n",
       "      <td>0.981095</td>\n",
       "      <td>0.964336</td>\n",
       "      <td>0.823259</td>\n",
       "      <td>0.965225</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.884611</td>\n",
       "      <td>0.143112</td>\n",
       "      <td>0.981095</td>\n",
       "      <td>0.964959</td>\n",
       "      <td>0.824313</td>\n",
       "      <td>0.966069</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701880</td>\n",
       "      <td>0.988616</td>\n",
       "      <td>0.969320</td>\n",
       "      <td>0.057625</td>\n",
       "      <td>0.240040</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.911708</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701285</td>\n",
       "      <td>0.989337</td>\n",
       "      <td>0.969320</td>\n",
       "      <td>0.057273</td>\n",
       "      <td>0.239534</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.913510</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.699857</td>\n",
       "      <td>0.989646</td>\n",
       "      <td>0.969320</td>\n",
       "      <td>0.056395</td>\n",
       "      <td>0.241222</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.915023</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.698549</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>0.969320</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.240547</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916826</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.696883</td>\n",
       "      <td>0.989749</td>\n",
       "      <td>0.969320</td>\n",
       "      <td>0.057625</td>\n",
       "      <td>0.242572</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.918668</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296000 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FIT101    LIT101    FIT201   DPIT301    LIT301    LIT401    FIT601  \\\n",
       "0       0.894061  0.145253  0.980632  0.966672  0.817110  0.964381  0.000037   \n",
       "1       0.888111  0.144064  0.981095  0.965737  0.818515  0.966238  0.000037   \n",
       "2       0.884144  0.144540  0.981095  0.964336  0.822205  0.965394  0.000037   \n",
       "3       0.883677  0.143826  0.981095  0.964336  0.823259  0.965225  0.000037   \n",
       "4       0.884611  0.143112  0.981095  0.964959  0.824313  0.966069  0.000037   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "295995  0.000000  0.701880  0.988616  0.969320  0.057625  0.240040  0.000037   \n",
       "295996  0.000000  0.701285  0.989337  0.969320  0.057273  0.239534  0.000037   \n",
       "295997  0.000000  0.699857  0.989646  0.969320  0.056395  0.241222  0.000037   \n",
       "295998  0.000000  0.698549  0.989749  0.969320  0.057800  0.240547  0.000037   \n",
       "295999  0.000000  0.696883  0.989749  0.969320  0.057625  0.242572  0.000037   \n",
       "\n",
       "        std_FIT101_120  std_LIT101_120  std_FIT201_120  ...  MV303_1  MV303_2  \\\n",
       "0                  0.0        0.000000        0.000000  ...      1.0      0.0   \n",
       "1                  0.0        0.000000        0.000000  ...      1.0      0.0   \n",
       "2                  0.0        0.000000        0.000000  ...      1.0      0.0   \n",
       "3                  0.0        0.000000        0.000000  ...      1.0      0.0   \n",
       "4                  0.0        0.000000        0.000000  ...      1.0      0.0   \n",
       "...                ...             ...             ...  ...      ...      ...   \n",
       "295995             0.0        0.911708        0.001927  ...      1.0      0.0   \n",
       "295996             0.0        0.913510        0.001909  ...      1.0      0.0   \n",
       "295997             0.0        0.915023        0.001897  ...      1.0      0.0   \n",
       "295998             0.0        0.916826        0.001889  ...      1.0      0.0   \n",
       "295999             0.0        0.918668        0.001879  ...      1.0      0.0   \n",
       "\n",
       "        MV304_0  MV304_1  MV304_2  P302_1  P302_2  P602_1  P602_2  \\\n",
       "0           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "1           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "2           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "3           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "4           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "...         ...      ...      ...     ...     ...     ...     ...   \n",
       "295995      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "295996      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "295997      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "295998      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "295999      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "\n",
       "        Normal/Attack  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "295995              0  \n",
       "295996              0  \n",
       "295997              0  \n",
       "295998              0  \n",
       "295999              0  \n",
       "\n",
       "[296000 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT101</th>\n",
       "      <th>LIT101</th>\n",
       "      <th>FIT201</th>\n",
       "      <th>DPIT301</th>\n",
       "      <th>LIT301</th>\n",
       "      <th>LIT401</th>\n",
       "      <th>FIT601</th>\n",
       "      <th>std_FIT101_120</th>\n",
       "      <th>std_LIT101_120</th>\n",
       "      <th>std_FIT201_120</th>\n",
       "      <th>...</th>\n",
       "      <th>MV303_1</th>\n",
       "      <th>MV303_2</th>\n",
       "      <th>MV304_0</th>\n",
       "      <th>MV304_1</th>\n",
       "      <th>MV304_2</th>\n",
       "      <th>P302_1</th>\n",
       "      <th>P302_2</th>\n",
       "      <th>P602_1</th>\n",
       "      <th>P602_2</th>\n",
       "      <th>Normal/Attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695099</td>\n",
       "      <td>0.989594</td>\n",
       "      <td>0.965893</td>\n",
       "      <td>0.055868</td>\n",
       "      <td>0.240209</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692482</td>\n",
       "      <td>0.989337</td>\n",
       "      <td>0.968229</td>\n",
       "      <td>0.054287</td>\n",
       "      <td>0.241391</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690221</td>\n",
       "      <td>0.989337</td>\n",
       "      <td>0.968541</td>\n",
       "      <td>0.055341</td>\n",
       "      <td>0.240378</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.688913</td>\n",
       "      <td>0.989337</td>\n",
       "      <td>0.968541</td>\n",
       "      <td>0.055341</td>\n",
       "      <td>0.241897</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687247</td>\n",
       "      <td>0.989337</td>\n",
       "      <td>0.970565</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>0.241897</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98995</th>\n",
       "      <td>0.896278</td>\n",
       "      <td>0.110992</td>\n",
       "      <td>0.981713</td>\n",
       "      <td>0.960754</td>\n",
       "      <td>0.742444</td>\n",
       "      <td>0.731937</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.062149</td>\n",
       "      <td>0.071088</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98996</th>\n",
       "      <td>0.892078</td>\n",
       "      <td>0.110754</td>\n",
       "      <td>0.981713</td>\n",
       "      <td>0.960754</td>\n",
       "      <td>0.742795</td>\n",
       "      <td>0.732612</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.061966</td>\n",
       "      <td>0.071199</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98997</th>\n",
       "      <td>0.886944</td>\n",
       "      <td>0.110516</td>\n",
       "      <td>0.982743</td>\n",
       "      <td>0.960754</td>\n",
       "      <td>0.743147</td>\n",
       "      <td>0.733456</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.062001</td>\n",
       "      <td>0.071265</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98998</th>\n",
       "      <td>0.884611</td>\n",
       "      <td>0.110635</td>\n",
       "      <td>0.982949</td>\n",
       "      <td>0.960754</td>\n",
       "      <td>0.744201</td>\n",
       "      <td>0.732443</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.062238</td>\n",
       "      <td>0.071426</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98999</th>\n",
       "      <td>0.884144</td>\n",
       "      <td>0.110397</td>\n",
       "      <td>0.982949</td>\n",
       "      <td>0.960754</td>\n",
       "      <td>0.744025</td>\n",
       "      <td>0.732950</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.062490</td>\n",
       "      <td>0.071516</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99000 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FIT101    LIT101    FIT201   DPIT301    LIT301    LIT401    FIT601  \\\n",
       "0      0.000000  0.695099  0.989594  0.965893  0.055868  0.240209  0.000037   \n",
       "1      0.000000  0.692482  0.989337  0.968229  0.054287  0.241391  0.000037   \n",
       "2      0.000000  0.690221  0.989337  0.968541  0.055341  0.240378  0.000037   \n",
       "3      0.000000  0.688913  0.989337  0.968541  0.055341  0.241897  0.000037   \n",
       "4      0.000000  0.687247  0.989337  0.970565  0.056219  0.241897  0.000037   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "98995  0.896278  0.110992  0.981713  0.960754  0.742444  0.731937  0.000073   \n",
       "98996  0.892078  0.110754  0.981713  0.960754  0.742795  0.732612  0.000073   \n",
       "98997  0.886944  0.110516  0.982743  0.960754  0.743147  0.733456  0.000073   \n",
       "98998  0.884611  0.110635  0.982949  0.960754  0.744201  0.732443  0.000073   \n",
       "98999  0.884144  0.110397  0.982949  0.960754  0.744025  0.732950  0.000073   \n",
       "\n",
       "       std_FIT101_120  std_LIT101_120  std_FIT201_120  ...  MV303_1  MV303_2  \\\n",
       "0            0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "1            0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "2            0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "3            0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "4            0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "...               ...             ...             ...  ...      ...      ...   \n",
       "98995        0.062149        0.071088        0.001501  ...      1.0      0.0   \n",
       "98996        0.061966        0.071199        0.001490  ...      1.0      0.0   \n",
       "98997        0.062001        0.071265        0.001493  ...      1.0      0.0   \n",
       "98998        0.062238        0.071426        0.001501  ...      1.0      0.0   \n",
       "98999        0.062490        0.071516        0.001508  ...      1.0      0.0   \n",
       "\n",
       "       MV304_0  MV304_1  MV304_2  P302_1  P302_2  P602_1  P602_2  \\\n",
       "0          0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "1          0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "2          0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "3          0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "4          0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "...        ...      ...      ...     ...     ...     ...     ...   \n",
       "98995      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "98996      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "98997      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "98998      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "98999      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "\n",
       "       Normal/Attack  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "...              ...  \n",
       "98995              0  \n",
       "98996              0  \n",
       "98997              0  \n",
       "98998              0  \n",
       "98999              0  \n",
       "\n",
       "[99000 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT101</th>\n",
       "      <th>LIT101</th>\n",
       "      <th>FIT201</th>\n",
       "      <th>DPIT301</th>\n",
       "      <th>LIT301</th>\n",
       "      <th>LIT401</th>\n",
       "      <th>FIT601</th>\n",
       "      <th>std_FIT101_120</th>\n",
       "      <th>std_LIT101_120</th>\n",
       "      <th>std_FIT201_120</th>\n",
       "      <th>...</th>\n",
       "      <th>MV303_1</th>\n",
       "      <th>MV303_2</th>\n",
       "      <th>MV304_0</th>\n",
       "      <th>MV304_1</th>\n",
       "      <th>MV304_2</th>\n",
       "      <th>P302_1</th>\n",
       "      <th>P302_2</th>\n",
       "      <th>P602_1</th>\n",
       "      <th>P602_2</th>\n",
       "      <th>Normal/Attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884144</td>\n",
       "      <td>0.110397</td>\n",
       "      <td>0.982949</td>\n",
       "      <td>0.960754</td>\n",
       "      <td>0.744025</td>\n",
       "      <td>0.732950</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.891145</td>\n",
       "      <td>0.110516</td>\n",
       "      <td>0.982949</td>\n",
       "      <td>0.960754</td>\n",
       "      <td>0.744025</td>\n",
       "      <td>0.732781</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.906779</td>\n",
       "      <td>0.110397</td>\n",
       "      <td>0.981713</td>\n",
       "      <td>0.957951</td>\n",
       "      <td>0.745431</td>\n",
       "      <td>0.732106</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.923230</td>\n",
       "      <td>0.110754</td>\n",
       "      <td>0.981713</td>\n",
       "      <td>0.957951</td>\n",
       "      <td>0.746836</td>\n",
       "      <td>0.731768</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.935947</td>\n",
       "      <td>0.112301</td>\n",
       "      <td>0.982022</td>\n",
       "      <td>0.957951</td>\n",
       "      <td>0.748066</td>\n",
       "      <td>0.733625</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449914</th>\n",
       "      <td>0.932563</td>\n",
       "      <td>0.100404</td>\n",
       "      <td>0.988616</td>\n",
       "      <td>0.992369</td>\n",
       "      <td>0.824664</td>\n",
       "      <td>0.797940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063371</td>\n",
       "      <td>0.058520</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449915</th>\n",
       "      <td>0.928596</td>\n",
       "      <td>0.103022</td>\n",
       "      <td>0.988616</td>\n",
       "      <td>0.992369</td>\n",
       "      <td>0.824840</td>\n",
       "      <td>0.799290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063253</td>\n",
       "      <td>0.059587</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449916</th>\n",
       "      <td>0.922179</td>\n",
       "      <td>0.103854</td>\n",
       "      <td>0.988873</td>\n",
       "      <td>0.989099</td>\n",
       "      <td>0.823435</td>\n",
       "      <td>0.799796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063258</td>\n",
       "      <td>0.060898</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449917</th>\n",
       "      <td>0.918446</td>\n",
       "      <td>0.103973</td>\n",
       "      <td>0.988873</td>\n",
       "      <td>0.987385</td>\n",
       "      <td>0.823259</td>\n",
       "      <td>0.801484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>0.062191</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449918</th>\n",
       "      <td>0.911329</td>\n",
       "      <td>0.105163</td>\n",
       "      <td>0.988100</td>\n",
       "      <td>0.986918</td>\n",
       "      <td>0.824137</td>\n",
       "      <td>0.801822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063169</td>\n",
       "      <td>0.063797</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449919 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FIT101    LIT101    FIT201   DPIT301    LIT301    LIT401    FIT601  \\\n",
       "0       0.884144  0.110397  0.982949  0.960754  0.744025  0.732950  0.000073   \n",
       "1       0.891145  0.110516  0.982949  0.960754  0.744025  0.732781  0.000073   \n",
       "2       0.906779  0.110397  0.981713  0.957951  0.745431  0.732106  0.000073   \n",
       "3       0.923230  0.110754  0.981713  0.957951  0.746836  0.731768  0.000073   \n",
       "4       0.935947  0.112301  0.982022  0.957951  0.748066  0.733625  0.000073   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "449914  0.932563  0.100404  0.988616  0.992369  0.824664  0.797940  0.000000   \n",
       "449915  0.928596  0.103022  0.988616  0.992369  0.824840  0.799290  0.000000   \n",
       "449916  0.922179  0.103854  0.988873  0.989099  0.823435  0.799796  0.000000   \n",
       "449917  0.918446  0.103973  0.988873  0.987385  0.823259  0.801484  0.000000   \n",
       "449918  0.911329  0.105163  0.988100  0.986918  0.824137  0.801822  0.000000   \n",
       "\n",
       "        std_FIT101_120  std_LIT101_120  std_FIT201_120  ...  MV303_1  MV303_2  \\\n",
       "0             0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "1             0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "2             0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "3             0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "4             0.000000        0.000000        0.000000  ...      1.0      0.0   \n",
       "...                ...             ...             ...  ...      ...      ...   \n",
       "449914        0.063371        0.058520        0.001667  ...      1.0      0.0   \n",
       "449915        0.063253        0.059587        0.001660  ...      1.0      0.0   \n",
       "449916        0.063258        0.060898        0.001654  ...      1.0      0.0   \n",
       "449917        0.063259        0.062191        0.001647  ...      1.0      0.0   \n",
       "449918        0.063169        0.063797        0.001641  ...      1.0      0.0   \n",
       "\n",
       "        MV304_0  MV304_1  MV304_2  P302_1  P302_2  P602_1  P602_2  \\\n",
       "0           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "1           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "2           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "3           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "4           0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "...         ...      ...      ...     ...     ...     ...     ...   \n",
       "449914      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "449915      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "449916      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "449917      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "449918      0.0      1.0      0.0     0.0     1.0     1.0     0.0   \n",
       "\n",
       "        Normal/Attack  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "449914              0  \n",
       "449915              0  \n",
       "449916              0  \n",
       "449917              0  \n",
       "449918              0  \n",
       "\n",
       "[449919 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1. División de los datos</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos los datos en función de si se trata de la variable clase o las variables predictoras::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns = \"Normal/Attack\")\n",
    "y_train = train[\"Normal/Attack\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = validation.drop(columns = \"Normal/Attack\")\n",
    "y_validation = validation[\"Normal/Attack\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(columns = \"Normal/Attack\")\n",
    "y_test = test[\"Normal/Attack\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Creación del modelo</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección explicaremos el modelo que hemos elegido (con su correspondiente justificación) debido a la naturaleza del problema y lo desarrollaremos paso a paso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1. Eligiendo el modelo</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los problemas de detección de anomalías, se recomienda utilizar un modelo de regresión de Deep Learning aplicado a una ventana W porque:\n",
    "\n",
    "- Modela comportamientos más complejos que el Machine Learning.\n",
    "- Se adecúa perfectamente a modelar datos de series temporales.\n",
    "\n",
    "En cuanto al tipo de problema, se debe optar por regresión en vez de clasificación debido a:\n",
    "\n",
    "- Señales y sensores de ICS siguen un patrón temporal fácilmente modeable por un regresor.\n",
    "- Adoptar un modelo de regresión junto a un umbral de anomalía genera información sobre el sensor que la causa, favoreciendo la interpretabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto al diseño de nuestro modelo, este recibirá una serie temporal con este formato: $\n",
    "(x_{0}, x_{1}, \\dots, x_{n-1})\n",
    "$.  A su vez,  $(y_{n+h}, \\dots, y_{n+h+m})$ es la salida del modelo, dónde n es la longitud de la ventana, m es número de predicciones futuras a realizar, h es el horizonte de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2. Eligiendo hiperparámetros</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es elegir los hiperparámetros del modelo. Estos se pueden dividir en dos grupos:\n",
    "\n",
    "- Hiperparámetros relacionados con los **datos de entrada**.\n",
    "- Hiperparámetros exclusivos del modelo de **Deep Learning**.\n",
    "\n",
    "Para configurar los hiperparámetros relacionados con los datos de entrada debemos tener en cuenta:\n",
    "\n",
    "- **n**: La ventana debe ser lo suficientemente grande para capturar el comportamiento del sistema. Se recomienda graficar las características para observar comportamientos repetitivos.\n",
    "- **h**: El horizonte de predicción especifica el paso de tiempo en el futuro a partir del cual el modelo comienza a predecir. Se sugiere probar diferentes valores para verificar que el modelo de Deep Learning está generalizando, en vez de replicar el último valor de entrada.\n",
    "- **m**: El número de pasos de tiempo predichos debe ajustarse según el escenario específico. En general, este valor se establecerá en uno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el modelo, utilizaremos la libería **pytorch**. Para cargar nuestros datos, tenemos que usar un dataset de la misma librería, por lo que creamos una clase personalizada para nuestro dataset. En esta clase, implementaremos un método que devuelva lotes de datos (para no tener mucha información a la vez en memoria) en tuplas x e y, representando series temporales hasta hasta $(y_{n+h}, \\dots, y_{n+h+m})$:.\n",
    "\n",
    "Primero, añadimos a nuestro código las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente creamos la clase personaliza de datos llamada **TimeSeriesDataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, n, h, m, overlap = 1):\n",
    "        \"\"\"\n",
    "        Parámetros:\n",
    "            data: Serie temporal, es un DataFrame.\n",
    "            n: Tamaño de la ventana, es un entero.\n",
    "            h: Horizonte de predicción, es un entero.\n",
    "            m: Número de predicciones futuras, es un entero.\n",
    "        \"\"\"\n",
    "\n",
    "        #lo converitmos a float32 para que el entrenamiento sea más rápido\n",
    "        if isinstance(data, np.ndarray):\n",
    "            self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        else:\n",
    "            self.data = torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "        self.n = n\n",
    "        self.h = h\n",
    "        self.m = m\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        #cantidad de muestras posibles del dataset\n",
    "        self.num_samples = (len(self.data) - (n + h + m) + 1)// self.overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        getitem: Devuelve la muestra correspondiente al índice dado.\n",
    "\n",
    "        Devuelve:\n",
    "            - x: ventana de tamaño n.\n",
    "            - y: un valor futuro después de un horizonte h (tamaño 1).\n",
    "        \"\"\"\n",
    "        real_idx = idx*self.overlap\n",
    "        #ventana de entrada de tamaño n, con lo que se entrena\n",
    "        x = self.data[real_idx:real_idx+self.n]\n",
    "\n",
    "        #queremos predecir el valor[n+m+h], por lo que lo comparamos con el real\n",
    "        y = self.data[real_idx+self.n+self.h:real_idx+self.n+self.h+self.m].reshape(-1)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, configuramos los hiperparámetros de los datos de entrada y dejamos preparados nuestros generadores de datos. Hemos decidido usar los siguientes hiperparámetros:\n",
    "\n",
    "- **n = 120**: Con una ventana de 120 segundos capturamos suficiente información sobre el pasado más reciente de los diferentes sensores y actuadores.\n",
    "- **h = 10**: Establecemos un valor mayor que uno debido a que las redes neuronales tienden a reproducir el último valor cuando el horizonte de predicción es cercano a la entrada.\n",
    "- **m = 1**: Solo nos interesa predecir el siguiente valor, por lo que lo dejamos a uno.\n",
    "\n",
    "En cuanto al tamaño de los lotes de nuestro generador, hemos decidido usar un batch de 5000 instancias, es decir, cada vez que el generador nos de datos será de 5000 en 5000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 120  #tamaño de la ventana\n",
    "h = 10   #horizonte de predicción\n",
    "m = 1    #número de predicciones futuras\n",
    "#batch_size = 295870\n",
    "\n",
    "#creación de los datasetsjupyter notebook\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, n, h, m)\n",
    "val_dataset = TimeSeriesDataset(X_validation, n, h, m)\n",
    "\n",
    "#creación de los dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    num_workers=2,      #para cargar los datos más rápido\n",
    "    pin_memory=True     #optimiza la transferencia de los datos a la GPU\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    num_workers=2,      #para cargar los datos más rápido\n",
    "    pin_memory=True     #optimiza la transferencia de los datos a la GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.3. Buscando los mejores hiperparámetros</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos elegido el modelo y hemos establecido los hiperparámetros de los datos de entrada, debemos de entrenar y hacer fine-tuning al modelo. En el artículo que estamos siguiendo se recomiendan dos enfoques:\n",
    "\n",
    "- **Grid Search**: Realiza una búsqueda exhaustiva en la que se prueban todas las posibles configuraciones de hiperparámetros y se queda con la mejor combinación. Es muy costosa en cuanto a tiempo pero asegura los mejores resultados dentro de los hiperparámetros definidos.\n",
    "- **Random Search**: Prueba valores aleatorios a partir de una lista de hiperparámetros predefinidos.\n",
    "\n",
    "En nuestro caso, realizaremos **Grid Search** y probaremos las posibles configuraciones de hiperparámetros con los datos de train. Para comprobar que tan buenas son esas configuraciones, debemos evaluarlas con los datos de validación. Para ello, como estamos en un problema de regresión, tenemos varios opciones para medir que tan buenas son:\n",
    "\n",
    "- Utilizar **MSE (Error Cuadrático Medio)** como error: Penaliza errores más grandes, útil para detectar anomalías más fuertes.\n",
    "- Utilizar **MSA (Error Absoluto Medio)** como error: Más robusto ante valores atípicos, útil si los datos tienen rudio.\n",
    "\n",
    "Nosotros utilizaremos el **MSE**, y nos quedaremos con el conjunto de hiperparámetros que tenga <u>menor error</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, mostramos los pasos necesarios para realizar el **Grid Search** correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero instalamos las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la función para construir el modelo. Algo que se debe tener en cuenta es que hemos decidido usar capas LSTM en este problema de detección de anomalías con serires temporales por las siguientes razones:\n",
    "\n",
    "- Las Redes Densas o RNNs estándar no pueden recordar información muy antigua debido al problema de <u>desvanecimiento del gradiente</u>.\n",
    "\n",
    "- Las celdas de las **LSTM (Long Short-Term Memory)** presentan puertas que permiten recordar información mucho más anterior que en una recurrente, consiguiendo capturar mejores patrones de largo plazo. Además, evita el problema de desvanecimiento del gradiente.\n",
    "\n",
    "- las celdas LSTM presentan mejor resultado que las GRU (Gated Recurrent Units), ya que son una versión simplifiada de las LSTM, aunque son más rápidas. Preferimos obtener mejor rendimiento a pesar de un mayor coste de entrenamiento.\n",
    "\n",
    "\n",
    "Nuestra red nueronal recibirá lotes de 5000 filas (batch) con todas las columnas del dataset, por lo que el tamaño de entrada de nuestra red deberá tener tamaño igual al número de características. La red se entrenará con los datos hasta la ventana de tiempo $ \\text{data}[n] $, y predecirá $\\text{data}[n+h+n]$. De esta manera, nuestro regresor devolverá una instancia con el valor predicho para todos los sensores y actuadores del dataset. Posteriormente, calcularemos el error con el valor real en ese instante de tiempo.\n",
    "\n",
    "\n",
    "Esta función recibe como parámetro la cantidad de capas y neuronas por capa LSTM y capa densa, además de la función de activación correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMPredictor(nn.Module):\n",
    "        def __init__(self, input_size, lstm_neurons, dense_neurons, activation, output_size):\n",
    "            super(LSTMPredictor, self).__init__()\n",
    "    \n",
    "            self.lstm_layers = nn.ModuleList()\n",
    "            self.dense_layers = nn.ModuleList()\n",
    "            #self.dropout = nn.Dropout(0.1) #dropout de 0.1 para evitar alto recall\n",
    "    \n",
    "            activations = {\n",
    "                'relu': nn.ReLU(),\n",
    "                'sigmoid': nn.Sigmoid()\n",
    "            }\n",
    "            self.activation = activations.get(activation, nn.ReLU())  # ReLU por defecto\n",
    "    \n",
    "            #para agregar las capas LSTM\n",
    "            for i in range(len(lstm_neurons)):\n",
    "                input_dim = input_size if i == 0 else lstm_neurons[i-1]\n",
    "                self.lstm_layers.append(nn.LSTM(input_dim, lstm_neurons[i], batch_first=True))\n",
    "    \n",
    "            #para agregar las capas densas\n",
    "            for i in range(len(dense_neurons)):\n",
    "                input_dim = lstm_neurons[-1] if i == 0 else dense_neurons[i-1]\n",
    "                self.dense_layers.append(nn.Linear(input_dim, dense_neurons[i]))\n",
    "    \n",
    "            #para generar la capa de salida\n",
    "            self.output_layer = nn.Linear(dense_neurons[-1], output_size)\n",
    "\n",
    "            #guardar la media, la desviación estándar y los errores en validación\n",
    "            self.register_buffer(\"train_mean\", torch.tensor(0.0))\n",
    "            self.register_buffer(\"train_std\", torch.tensor(1.0))\n",
    "            self.register_buffer(\"val_errors\", torch.tensor([]))\n",
    "    \n",
    "        def forward(self, x):\n",
    "            #pasa por las capas LSTM\n",
    "            for lstm in self.lstm_layers:\n",
    "                x, _ = lstm(x)\n",
    "                #x = self.dropout(x) #después de cada capa LSTM hacemos dropout\n",
    "            #tomamos el último estado\n",
    "            x = x[:, -1, :]\n",
    "    \n",
    "            #pasa por las capas densas\n",
    "            for dense in self.dense_layers:\n",
    "                x = self.activation(dense(x))\n",
    "                #x = self.dropout(x) #despues de cada capa Densa hacemos dropout\n",
    "            #capa de salida\n",
    "            x = self.output_layer(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, lstm_neurons, dense_neurons, activation, output_size):\n",
    "    \"\"\"\n",
    "    build_model: Construye un modelo LSTM en PyTorch para regresión de series temporales.\n",
    "\n",
    "    Parámetros:\n",
    "        input_size: Número de características de entrada, es un entero.\n",
    "        lstm_neurons: Lista con el número de neuronas en cada capa LSTM, es una lista.\n",
    "        dense_neurons: Lista con el número de neuronas en cada capa densa, es una lista.\n",
    "        activation: Función de activación de las capas densas ('relu', 'sigmoid'), es una lista.\n",
    "        output_size: Número de características de salida, es un entero.\n",
    "\n",
    "    Devuelve:\n",
    "        Modelo preparado para entrenar.\n",
    "    \"\"\"\n",
    "    return LSTMPredictor(input_size, lstm_neurons, dense_neurons, activation, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso que debemos realizar es configurar el espacio de búsqueda de hiperparámetros. Para ello, vamos a crear un diccionario en el que vamos a establecer:\n",
    "\n",
    "- Número de nueronas por capa LSTM. Comprobaremos si es mejor con 3 o 4 capas.\n",
    "- Número de neuronas por capa densa. Comprobaremos si es mejor con 1 o 2 capas.\n",
    "- Función de activación. Comprobaremos si es mejor la relu o la sigmoide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: dividimos en dos diccionarios para evitar problemas de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "    'lstm_neurons': [[512, 256, 130], [256, 130]],\n",
    "    'dense_neurons':  [[704], [704, 200]],\n",
    "    'activation': ['relu'],\n",
    "    'epochs': [3] #utilizamos 5 épocas para realizar el grid_search\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {\n",
    "    'lstm_neurons': [[512, 256, 130], [256, 130]],\n",
    "    'dense_neurons':  [[704], [704, 200]],\n",
    "    'activation': ['sigmoid'],\n",
    "    'epochs': [3] #utilizamos 5 épocas para realizar el grid_search\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente creamos el código para realizar el **Grid Search**. Explicamos detalladamente como funciona el código para un mayor entendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usamos optimización cuDNN para acelerar el entrenamiento\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(build_model, param_grid, train_loader, val_loader,output_size, device='cuda'):\n",
    "    \"\"\"\n",
    "    grid_search: Función que realizar un grid search de manera manual. Entrena 4 épocas cada posible modelo en función\n",
    "    de los hiperparámetros dados, para posteriormente evaluarlo. Esta función guarda cual es la mejor combinación de\n",
    "    hiperparámetros y los devuelve.\n",
    "\n",
    "    Parámetros:\n",
    "    build_model: Función que hemos diseñado para construir el modelo de forma dinámica. Recibe la lista con las\n",
    "    neuronas correspondientes y devuelve el modelo, es una función.\n",
    "    param_grid: Diccionario que contiene los hiperparámetros a probar en la búsqueda, es un diccionario.\n",
    "    train_loader: Es el DataLoader que hemos creado para cargar los datos de entrenamiento, es un DataLoader.\n",
    "    val_loader: Es el DataLoader que hemos creado para cargar los datos de validación, es un DataLoader.\n",
    "    device='cpu': Dispositivo dónde se entrena el modelo, por defecto es la cpu.\n",
    "    output_size: Tamaño de la capa de salida, es un entero.\n",
    "    \n",
    "    Devuelve:\n",
    "        - best_params: diccionario que contiene la mejor combinación de hiperparámetros que ha encontrado el modelo.\n",
    "        - best_score: valor entero que indica el mejor score del mejor modelo.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #comenzamos instanciando el mejor score a valor infinito,\n",
    "    #ya que nuestra métrica es el MSE y nos queremos quedar con el modelo\n",
    "    #que ofrezca el menor error cuadrático medio\n",
    "    best_score = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    #obtenemos un batch de los datos de train loader\n",
    "    X_sample, _ = next(iter(train_loader))\n",
    "    #establecemos el tamaño de la entrada de la red a la cantidad de columnas de \n",
    "    #nuestro dataset, X_sample tiene un tamaño [5000, 105], ([tam_batch,columns])\n",
    "    #nuestra red tendrá un tamaño de entrada de 105\n",
    "    input_size = X_sample.shape[2]\n",
    "\n",
    "    #recorremos todas las configuraciones posibles del diccionario param_grid\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"\\n Probando configuración: {params}\")\n",
    "\n",
    "        #llamamos a nuestra función build_model para crear el modelo\n",
    "        #con los hiperparámetros de la iteración actual del param_grid\n",
    "        model = build_model(\n",
    "            input_size=input_size,\n",
    "            lstm_neurons=params['lstm_neurons'],\n",
    "            dense_neurons=params['dense_neurons'],\n",
    "            activation=params['activation'],\n",
    "            output_size = output_size\n",
    "        ).to(device)\n",
    "\n",
    "        #utilizamos el optimizador Adam con learning rate 0.0001\n",
    "        #para evitar el problea de explosión de gradiente\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "        #establecemos que la función de pérdida que queremos usar\n",
    "        #es el error absoluto medio (MAE)\n",
    "        loss_fn = nn.L1Loss()\n",
    "\n",
    "        print(f\" Entrenando con epochs = {params['epochs']}\")\n",
    "\n",
    "        #realizamos tantas épocas como se establezca en el diccionario de param_grid,\n",
    "        #nosotros entrenamos con 4 épocas\n",
    "        for epoch in range(params['epochs']):\n",
    "            model.train() #activar el modo de entrenamiento\n",
    "            train_loss = 0.0 #inicializamos la pérdida\n",
    "\n",
    "            #utilizamos progress_bar para tener un poco más de información durante el entrenamiento\n",
    "            progress_bar = tqdm(train_loader, desc=f\" Epoch {epoch + 1}/{params['epochs']}\", leave=False)\n",
    "\n",
    "            #para todos los batches durante esta época, convertimos los datos a float32 para\n",
    "            #que el entrenamiento sea más rápido\n",
    "            #nota: pogress_bar es una versión del train_loader, por lo que recorremos\n",
    "            #el train por todos los batches que genere:\n",
    "            for X_batch, y_batch in progress_bar:\n",
    "                X_batch = X_batch.to(device).float()\n",
    "                y_batch = y_batch.to(device).float()\n",
    "\n",
    "                #reseteamos los gradientes para realizar el entrenamiento\n",
    "                #en la epoch actual\n",
    "                optimizer.zero_grad()\n",
    "                #obtenemos las predicciones del modelo\n",
    "                y_pred = model(X_batch)\n",
    "                #y_batch tiene forma [5000,1,105], hay que eliminar\n",
    "                y_batch = y_batch.view(y_batch.shape[0], -1) #asegura que siempre sea (batch_size, m)\n",
    "                #calculamos la pérdida actual usando MSE\n",
    "                loss = loss_fn(y_pred, y_batch) # Calcular la pérdida\n",
    "                #hacemos la retropropagación correspondiente\n",
    "                loss.backward()\n",
    "                #actualizamos los parámetros del modelo\n",
    "                optimizer.step()\n",
    "                #aumentamos la pérdida del entrenamiento\n",
    "                train_loss += loss.item()\n",
    "                #actualizamos la barra de progreso\n",
    "                progress_bar.set_postfix(loss=f\"{train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "            print(f\" Epoch {epoch + 1}/{params['epochs']} - Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval() #activar el modo de evaluación\n",
    "        #preparamos listas vacías para guardar las predicciones y valores reales\n",
    "        predictions, true_values = [], []\n",
    "        print(\" Evaluando en conjunto de validación...\")\n",
    "        #desactivamos el cálculo de graidentes, para hacer predicciones\n",
    "        #no necesitamos hacer retropropagación\n",
    "        with torch.no_grad():\n",
    "            #recorremos todo el dataset de validación por batches:\n",
    "            for X_val, y_val in val_loader:\n",
    "                \n",
    "                #convertimos los datos a float32 para que el entrenamiento\n",
    "                #sea más rápido\n",
    "                X_val, y_val = X_val.to(device).float(), y_val.to(device).float()\n",
    "                #reducimos en 1 la dimensión innecesaria como en el entrenamiento\n",
    "                #y_val = y_val.squeeze(1)\n",
    "                y_val = y_val.view(y_val.shape[0], -1)  # Asegurar correcta forma\n",
    "                #obtenemos la predicción del modelo\n",
    "                y_pred = model(X_val)\n",
    "                #añadimos las predicciones a las listas definidas\n",
    "                #lo hacemos como array de NumPy para después\n",
    "                #calcular mean_absolute_error\n",
    "                predictions.append(y_pred.cpu().numpy())\n",
    "                true_values.append(y_val.cpu().numpy())\n",
    "                \n",
    "        #concatenamos todas las predicciones y los valores reales\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "        #obtenemos el score del modelo utilizando la función de\n",
    "        #NumPy para calcular el Error Cuadrático Medio\n",
    "        score = mean_absolute_error(true_values, predictions)\n",
    "        print(f\" MAE en validación: {score:.4f}\")\n",
    "\n",
    "        #si el score actual es menor que el mejor score guardado,\n",
    "        #lo almacena\n",
    "        #actualiza también los mejores hiperparámetros\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "        #elimina el modelo de la memoria para no tener problemas de memoria\n",
    "        del model\n",
    "        #libera la memoria de la GPU para evitar guardar datos no utilizados\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n Mejor configuración: {best_params}\")\n",
    "    print(f\" Mejor MAE: {best_score:.4f}\")\n",
    "\n",
    "    #devuelve los mejores parámetros y el mejor score\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Probando configuración: {'activation': 'relu', 'dense_neurons': [704], 'epochs': 3, 'lstm_neurons': [512, 256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0111\n",
      " Evaluando en conjunto de validación...\n",
      " MAE en validación: 0.0920\n",
      "\n",
      " Probando configuración: {'activation': 'relu', 'dense_neurons': [704], 'epochs': 3, 'lstm_neurons': [256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0072\n",
      " Evaluando en conjunto de validación...\n",
      " MAE en validación: 0.0577\n",
      "\n",
      " Probando configuración: {'activation': 'relu', 'dense_neurons': [704, 200], 'epochs': 3, 'lstm_neurons': [512, 256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0158\n",
      " Evaluando en conjunto de validación...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAE en validación: 0.0648\n",
      "\n",
      " Probando configuración: {'activation': 'relu', 'dense_neurons': [704, 200], 'epochs': 3, 'lstm_neurons': [256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0131\n",
      " Evaluando en conjunto de validación...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAE en validación: 0.0916\n",
      "\n",
      " Mejor configuración: {'activation': 'relu', 'dense_neurons': [704], 'epochs': 3, 'lstm_neurons': [256, 130]}\n",
      " Mejor MAE: 0.0577\n"
     ]
    }
   ],
   "source": [
    "if run_grid:\n",
    "    params1, score1 = grid_search(build_model, param_grid1, train_loader, val_loader, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Probando configuración: {'activation': 'sigmoid', 'dense_neurons': [704], 'epochs': 3, 'lstm_neurons': [512, 256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0120\n",
      " Evaluando en conjunto de validación...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAE en validación: 0.0914\n",
      "\n",
      " Probando configuración: {'activation': 'sigmoid', 'dense_neurons': [704], 'epochs': 3, 'lstm_neurons': [256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0094\n",
      " Evaluando en conjunto de validación...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAE en validación: 0.0913\n",
      "\n",
      " Probando configuración: {'activation': 'sigmoid', 'dense_neurons': [704, 200], 'epochs': 3, 'lstm_neurons': [512, 256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0180\n",
      " Evaluando en conjunto de validación...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MAE en validación: 0.1953\n",
      "\n",
      " Probando configuración: {'activation': 'sigmoid', 'dense_neurons': [704, 200], 'epochs': 3, 'lstm_neurons': [256, 130]}\n",
      " Entrenando con epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/3 - Loss: 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 2/3 - Loss: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 3/3 - Loss: 0.0183\n",
      " Evaluando en conjunto de validación...\n",
      " MAE en validación: 0.1691\n",
      "\n",
      " Mejor configuración: {'activation': 'sigmoid', 'dense_neurons': [704], 'epochs': 3, 'lstm_neurons': [256, 130]}\n",
      " Mejor MAE: 0.0913\n"
     ]
    }
   ],
   "source": [
    "if run_grid:\n",
    "    params2, score2 = grid_search(build_model, param_grid2, train_loader, val_loader, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if score1 < score2:\n",
    "        best_grid_search = params1\n",
    "else:\n",
    "        best_grid_search = params2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez conocemos cuál es la mejor combinación de capas LSTM y capas densas, creamos el diccionario para poder entrenar nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = {\n",
    "    'lstm_neurons': best_grid_search['lstm_neurons'],\n",
    "    'dense_neurons': best_grid_search['dense_neurons'],\n",
    "    'activation': best_grid_search['activation'],\n",
    "    'epochs': 9 #aquí especificamos el número de épocas para entrenar el modelo\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.4. Entrenando el modelo</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez completados los pasos anteriores, es momento de entrenar el modelo. A continuación, explicamos los pasos necesarios en función de las recomendaciones del artículo que estamos siguiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero, unimos los datos de train y test para entrenar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que para hacer el grid search, debemos crear nuestros generadores de datos tanto para los datos de entrenamiento como para los datos de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creación de los datasets tanto para entrenamiento como para test\n",
    "test_dataset = TimeSeriesDataset(X_test, n, h, m)\n",
    "\n",
    "#creación de los dataloaders\n",
    "\n",
    "\n",
    "#creamos el dataloader para los datos de test\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    num_workers=2,      #para cargar los datos más rápido\n",
    "    pin_memory=True     #optimiza la transferencia de los datos a la GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos los datos preparados y la configuración del modelo que vamos usar, es momento de crear la función que se encargará de entrenar el modelo. Explicamos cada paso mediante los comentarios en el código correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcular el error entre el resultado predicho por la red nueronal $\\hat{y}$ y el valor real en el dataset $y$ usamos el valor absoluto:\n",
    "\n",
    "$e = \\left| y - \\hat{y} \\right|$\n",
    "\n",
    "De este paso, se encarga la función **computa_error**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computa_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    computa_error: Calcula el error absoluto entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Parámetros:\n",
    "    y_true: Vector que contien los valores reales del dataset, es un array de NumPy.\n",
    "    y_pred: Vector que contiene las predicciones realiadas por la red neuronal, es un array de NumPy.\n",
    "    \n",
    "    Devuelve:\n",
    "        -error absoluto entre el vector de predicciones y el de valores reales.\n",
    "    \"\"\"\n",
    "    error = torch.abs(y_true - y_pred).detach().cpu().numpy() #error por variable, no se hace la media\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(build_model, best_grid, train_loader, val_loader, output_size, device='cpu'):\n",
    "    \"\"\"\n",
    "    train_model: Función que entrena el modelo de detección de anomalías en Pytorch y que guarda dicho modelo.\n",
    "\n",
    "    Parámetros:\n",
    "    build_model: Función que hemos diseñado para construir el modelo de forma dinámica. Recibe la lista con las neuronas\n",
    "    correspondientes y devuelve el modelo, es una función.\n",
    "    best_grid: Diccionario que contiene la mejor combinación de hiperparámetros, es un diccionario.\n",
    "    train_loader: Es el DataLoader que hemos creado para cargar los datos de entrenamiento, es un DataLoader.\n",
    "    val_loader: Es el DataLoader que hemos creado para cargar los datos de validación, es un DataLoader.\n",
    "    output_size: Es el tamaño de la capa de salida, es un entero.\n",
    "    \n",
    "    Devuelve:\n",
    "        Entrena el modelo y guarda los modelos en las épocas 5,10,15 y el modelo final.\n",
    "\n",
    "    \n",
    "    modificación 20 marzo 2025:\n",
    "    -añado que se calcule durante el entrenamiento el error en cada epoch para luego calcular la media y desviación\n",
    "     estándar del train y guardarlo en el modelo. De esta manera, no tenemos que volver a pasar el train en la evaluación.\n",
    "    \"\"\"\n",
    "    #obtenemos un batch de los datos de train loader\n",
    "    X_sample, _ = next(iter(train_loader))\n",
    "    #establecemos el tamaño de la entrada de la red a la cantidad\n",
    "    #de columnas de nuestro dataset, al igual que en la función\n",
    "    #del grid_search\n",
    "    input_size = X_sample.shape[2]\n",
    "\n",
    "    #creamos el modelo con la función build_model y el diccionario\n",
    "    #de best_params\n",
    "    model = build_model(\n",
    "            input_size=input_size,\n",
    "            lstm_neurons=best_grid['lstm_neurons'],\n",
    "            dense_neurons=best_grid['dense_neurons'],\n",
    "            activation=best_grid['activation'],\n",
    "            output_size=output_size\n",
    "    ).to(device)\n",
    "\n",
    "    #utilizamos el optimizador Adam con learning rate 0.0001\n",
    "    #para evitar el problea de explosión de gradiente\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    #establecemos que la función de pérdida que queremos usar\n",
    "    #es el error cuadrático medio (MSE)\n",
    "    loss_fn = nn.L1Loss()\n",
    "\n",
    "    #realizamos tantas épocas como indique el diccionario best_grid\n",
    "    for epoch in range(best_grid['epochs']):\n",
    "        #activamos el modo de entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0 #inicializamos la pérdida\n",
    "        train_errors = [] #lista para errores del train\n",
    "        #usamos progress_bar para tener un pco más de información durante el entrenamiento\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{best_grid['epochs']}\", leave=False)\n",
    "\n",
    "        #para todos los batches durante esta época, convertimos los datos a float32 para\n",
    "        #que el entrenamiento sea más rápido\n",
    "        #nota: pogress_bar es una versión del train_loader, por lo que recorremos\n",
    "        #el train por todos los batches que genere:\n",
    "        for X_batch, y_batch in progress_bar:\n",
    "            X_batch = X_batch.to(device).float()\n",
    "            y_batch = y_batch.to(device).float()\n",
    "            #reseteamos los gradientes para realizar el entrenamiento\n",
    "            #en la epoch actual\n",
    "            optimizer.zero_grad()\n",
    "            #obtenemos las predicciones del modelo\n",
    "            y_pred = model(X_batch)\n",
    "            #hay que asegurnaos que tenga la forma (batch_size, m)\n",
    "            y_batch = y_batch.view(y_batch.shape[0], -1)\n",
    "            #calculamos la pérdida actual usando MSE\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            #hacemos la retropropagación correspondiente\n",
    "            loss.backward()\n",
    "            #actualizamos los parámetros del modelo\n",
    "            optimizer.step()\n",
    "            #aumentamos la pérdida del entrenamiento\n",
    "            train_loss += loss.item()\n",
    "\n",
    "\n",
    "            #actualizamos la barra de progreso\n",
    "            progress_bar.set_postfix(loss=f\"{train_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        #nuevo 20/03/2025:\n",
    "        #para calcular los errores debe ser sólo en la última época\n",
    "        if epoch == best_grid['epochs']- 1:\n",
    "            train_errors, val_errors = evalua_y_guarda_results(model, train_loader, val_loader, device)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{best_grid['epochs']} - Loss: {train_loss / len(train_loader):.4f}\")\n",
    "        #guardamos los modelos en las épocas 1,2 y 3:\n",
    "        if epoch + 1 in [2,4,5,6]:  \n",
    "            model_path = f\"checkpoint_{epoch + 1}.pt\"\n",
    "            scripted_model = torch.jit.script(model)\n",
    "            torch.jit.save(scripted_model, model_path)\n",
    "            print(f\"Checkpoint guardado en: {model_path}\")\n",
    "\n",
    "    #nuevo 20/33/2025:\n",
    "    train_errors = np.concatenate(train_errors,axis=0)\n",
    "    model.train_mean = torch.tensor(np.mean(train_errors)).to(device)\n",
    "    model.train_std = torch.tensor(np.std(train_errors)).to(device)\n",
    "    model.val_errors = torch.tensor(val_errors).to(device)\n",
    "\n",
    "    #guardamos el modelo final\n",
    "    final_model_path = \"modelo_completo.pt\"\n",
    "    scripted_model = torch.jit.script(model)\n",
    "    torch.jit.save(scripted_model, final_model_path)\n",
    "    print(f\"Modelo entrenado guardado en: {final_model_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_y_guarda_results(model, train_loader, val_loader, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    calcula los errores en la última epoch tanto para train como para validación\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    train_errors = []\n",
    "    print(\"Calculando los errores en el entrenamiento...\")\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=\"Calculando errores en entrenamiento\", leave= False):\n",
    "            X_batch = X_batch.to(device).float()\n",
    "            y_batch = y_batch.to(device).float()\n",
    "            y_pred = model(X_batch)\n",
    "            error = computa_error(y_batch, y_pred)\n",
    "            train_errors.append(error)\n",
    "    \n",
    "    train_errors = np.concatenate(train_errors, axis = 0)\n",
    "    print(\"Calculando los errores en validación...\")\n",
    "    val_errors = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(val_loader, desc=\"Calculando errores en validación\", leave= False):\n",
    "            X_batch = X_batch.to(device).float()\n",
    "            y_batch = y_batch.to(device).float()\n",
    "            y_pred = model(X_batch)\n",
    "            error = computa_error(y_batch, y_pred)\n",
    "            error_tensor = torch.tensor(error).to(device)  # Convertir el error a tensor de PyTorch\n",
    "            val_errors.append(error_tensor)\n",
    "\n",
    "    # Concatenar todos los errores de validación\n",
    "    val_errors_tensor = torch.cat(val_errors, dim=0)\n",
    "\n",
    "    return train_errors, val_errors_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9 - Loss: 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/9 - Loss: 0.0083\n",
      "Checkpoint guardado en: checkpoint_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/9 - Loss: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/9 - Loss: 0.0064\n",
      "Checkpoint guardado en: checkpoint_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/9 - Loss: 0.0061\n",
      "Checkpoint guardado en: checkpoint_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/9 - Loss: 0.0062\n",
      "Checkpoint guardado en: checkpoint_6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/9 - Loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/9 - Loss: 0.0058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando los errores en el entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando los errores en validación...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_906249/1866981663.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model.val_errors = torch.tensor(val_errors).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/9 - Loss: 0.0072\n",
      "Modelo entrenado guardado en: modelo_completo.pt\n"
     ]
    }
   ],
   "source": [
    "train_model(build_model, best_grid, train_loader, val_loader, X_train.shape[1], device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Evaluación del modelo</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se debe calcular el error par el conjunto de entrenamiento. Calculamos la **media** del error y su **desviación estándar**. Luego, calculamos el error con los datos del conjunto de validación. \n",
    "En nuestro caso, utilizaremos como conjunto de validación los datos que originalmente habíamos reservado a dicho conjunto.\n",
    "\n",
    "El siguiente paso será calcular el **Z-score** para el conjunto de datos de validación usando la siguiente fórmula:\n",
    "\n",
    "$z_e = \\frac{e_v - \\mu_e}{\\sigma_e}$\n",
    "\n",
    "Siendo $\\mu_e$ la media del error en los datos de entrenamiento y $\\sigma_e$ la desviación estándar en los datos de entrenamiento.\n",
    "\n",
    "Una vez calculamos los errores en validación, debemos definir un umbral para decidir si se trata de una anomalía o no. En nuestro caso, calcularemos un umbral por característica a partir máximo (tras haber realizado un estudio que mostraremos a continuación) del Z-score calculado en validación. Si para una muestra en el test tiene mayor score al umbral anterior, se considerará anomalía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, test_loader, y_test_true, device='cpu'):\n",
    "    \"\"\"\n",
    "    evaluate_model: Obtiene los errores con los datos de entrenamiento, calcula el umbral con los datos\n",
    "    de validación y obtiene los resultados del modelo con los datos del test.\n",
    "    \n",
    "    Parámetros:\n",
    "    model_path: ruta para cargar el modelo a evualuar.    \n",
    "    test_loader: Es el DataLoader que hemos creado para cargar los datos de test, es un DataLoader.\n",
    "    y_test_true: Vector que contiene los valores de la etiqueta clase de los datos de test, es un vector de NumPy.\n",
    "\n",
    "\n",
    "    modificación 20 marzo 2025:\n",
    "    -elimino el cálculo de la media y s.d. de error del train ya que ahora se calcula durante el entrenamiento.\n",
    "    -elimino el cálculo de val_errors y lo añado en el entrenamiento.\n",
    "    \n",
    "    \"\"\"\n",
    "    #cargamos el modelo guardado\n",
    "    model = torch.jit.load(model_path, map_location=device)\n",
    "    print(f\"Modelo cargado desde {model_path}\")\n",
    "    \n",
    "    #activamos el modo de evaluación\n",
    "    model.eval()\n",
    "    \n",
    "    #nuevo 20/03/2025:\n",
    "    #se lee la media la s.d. guardados en el modelo\n",
    "    train_mean = model.train_mean.cpu().numpy()\n",
    "    train_std = model.train_std.cpu().numpy()\n",
    "    val_errors = model.val_errors.cpu().numpy()\n",
    "\n",
    "\n",
    "    #calculamos los z-score en el conjunto de validación\n",
    "    val_z_scores = (val_errors - train_mean) / train_std\n",
    "\n",
    "    #definimos el umbral para detectar anomalías: percentil 99.5%\n",
    "    feature_thresholds = np.max(val_z_scores, axis=0)#umbral por variable\n",
    "\n",
    "    #en los datos de test, hacemos lo mismo\n",
    "    #definimos array vacío para los errores en el test\n",
    "    test_errors = []\n",
    "    test_predictions = []\n",
    "    y_test_real = []\n",
    "    #desactivamos el cálculo de gradientes\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in tqdm(test_loader):\n",
    "            X_batch = X_batch.to(device).float()\n",
    "            y_batch = y_batch.to(device).float()\n",
    "            y_pred = model(X_batch)\n",
    "            #calculamos el error en el test con la función computa_error\n",
    "            error = computa_error(y_batch, y_pred)\n",
    "            test_errors.append(error)\n",
    "            test_predictions.append(y_pred.cpu().numpy()) #guardo predicciones\n",
    "            y_test_real.append(y_batch.cpu().numpy())\n",
    "            \n",
    "    #concatenamos todos los errores de test\n",
    "    test_errors = np.concatenate(test_errors, axis=0)\n",
    "    test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "    y_test_real = np.concatenate(y_test_real, axis=0)\n",
    "    print(f\"Longitud de test_errors: {len(test_errors)}\")\n",
    "\n",
    "    #Calculamos el z-score en el conjunto de prueba\n",
    "    test_z_scores = (test_errors - train_mean) / train_std\n",
    "\n",
    "    #detectamos las anomalías usando el umbral definido en validación\n",
    "    anomalies_por_feature = test_z_scores > feature_thresholds #matriz de anomalias\n",
    "\n",
    "    #Crear un array con las dos primeras variables que causaron anomalía por fila\n",
    "    # Si hay menos de dos variables que causan anomalía, devolvemos solo las que existan\n",
    "    anomalies_list = [np.where(row)[0].tolist() if row.any() else [] for row in anomalies_por_feature]\n",
    "\n",
    "    # Convertir la detección de anomalías a binario (si alguna variable es anomalía, la fila es anomalía)\n",
    "    anomalies = anomalies_por_feature.any(axis=1).astype(int)\n",
    "\n",
    "    print(f\"Anomalías detectadas en el conjunto de prueba: {np.sum(anomalies)}\")\n",
    "\n",
    "\n",
    "    # Ajustar dimensiones\n",
    "    min_length = min(len(anomalies), len(y_test_true), len(test_predictions))\n",
    "    anomalies = anomalies[:min_length]\n",
    "    anomalies_list = anomalies_list[:min_length]\n",
    "    y_test_true = y_test_true[:min_length]\n",
    "    test_predictions = test_predictions[:min_length]\n",
    "    y_test_real = y_test_real[:min_length]\n",
    "\n",
    "    print(f\"Shape corregido de anomalies: {len(anomalies)}\")\n",
    "    print(f\"Shape corregido de y_test_true: {len(y_test_true)}\")\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_results = pd.DataFrame({\n",
    "        'Predicción': list(test_predictions),  # Guardamos las predicciones del modelo\n",
    "        'Anomalía': anomalies,  # 1 si es anomalía, 0 si no lo es\n",
    "        'Variables_Anómalas': anomalies_list  # Lista de variables que causan la anomalía\n",
    "    })\n",
    "    #llamamos a la función calcula_metricas para conocer el redimiento del modelo\n",
    "    print(\"Calculando métricas de rendimiento para el conjunto de prueba...\")\n",
    "    calcula_metricas(y_test_true, anomalies)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder evaluar el modelo, debemos usar diferentes métricas de evaluación. En nuestro caso, nos enfocaremos en:\n",
    "\n",
    "- **Precisión**: Esta métrica nos ayuda a conocer si nuestro modelo tiene muchos falsos positivos.\n",
    "- **Recall**: Esta métrica indica el número de anomalías predichas por nuestro modelo en función de las anomalías totales.\n",
    "\n",
    "Buscaremos un equilibrio entre dichas métricas y nos fijaremos en el **F1-Score**. \n",
    "También mostraremos el accuracy y la matriz de confusión para tener más información. De este paso se ocupará la función **calcula_metricas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_metricas(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    calcula_metricas: Calcula e imprime las métricas de Precision, Recall, F1-Score, Accuracy y la matriz de confusión.\n",
    "\n",
    "    Parámetros:\n",
    "    true_labels: Etiquetas reales de anomalías (1 para anomalía, 0 para no anomalía), es un vector.\n",
    "    predicted_labels: Etiquetas predichas por el modelo (1 para anomalía, 0 para no anomalía), es un vector.\n",
    "\n",
    "    Devuelve:\n",
    "        -Precisión del modelo.\n",
    "        -Recall del modelo.\n",
    "        -F1-Score del modelo.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    #para imprimir la matriz de confusión\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Anomalía\"], yticklabels=[\"Normal\", \"Anomalía\"])\n",
    "    plt.xlabel(\"Predicción\")\n",
    "    plt.ylabel(\"Valor Real\")\n",
    "    plt.title(\"Matriz de Confusión\")\n",
    "    plt.show()\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras haber ejecutado los varios modelos entrenados y haber probado diferentes combinaciones de umbrales, mostramos los resultados obtenidos en un estudio realizado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con el modelo de **10 epochs** con un umbral de **99,92** debido a que es el más estable de todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real =y_test.values[130:]\n",
    "y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado desde modelo_completo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 449789/449789 [1:32:16<00:00, 81.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de test_errors: 449789\n",
      "Anomalías detectadas en el conjunto de prueba: 50964\n",
      "Shape corregido de anomalies: 449789\n",
      "Shape corregido de y_test_true: 449789\n",
      "Calculando métricas de rendimiento para el conjunto de prueba...\n",
      "Precision: 0.7726\n",
      "Recall: 0.7208\n",
      "F1-Score: 0.7458\n",
      "Accuracy: 0.9403\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_resultados = evaluate_model(\"modelo_completo.pt\",test_loader,y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicción</th>\n",
       "      <th>Anomalía</th>\n",
       "      <th>Variables_Anómalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.96810484, 0.15293278, 0.988762, 0.97091335,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.9725529, 0.1533977, 0.9884022, 0.9720329, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.97983277, 0.15336007, 0.98861706, 0.9731666...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.9859836, 0.15315332, 0.9889125, 0.973971, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.99003935, 0.1530394, 0.98909307, 0.9744873,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449784</th>\n",
       "      <td>[0.94011855, 0.14096266, 0.98932415, 0.9844897...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449785</th>\n",
       "      <td>[0.93586564, 0.14096743, 0.9892238, 0.98398304...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449786</th>\n",
       "      <td>[0.92942595, 0.1409559, 0.9890476, 0.98311985,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449787</th>\n",
       "      <td>[0.9255427, 0.14109313, 0.9888827, 0.98237425,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449788</th>\n",
       "      <td>[0.92524135, 0.14110829, 0.98887056, 0.9822847...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449789 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Predicción  Anomalía  \\\n",
       "0       [0.96810484, 0.15293278, 0.988762, 0.97091335,...         0   \n",
       "1       [0.9725529, 0.1533977, 0.9884022, 0.9720329, 0...         0   \n",
       "2       [0.97983277, 0.15336007, 0.98861706, 0.9731666...         0   \n",
       "3       [0.9859836, 0.15315332, 0.9889125, 0.973971, 0...         0   \n",
       "4       [0.99003935, 0.1530394, 0.98909307, 0.9744873,...         0   \n",
       "...                                                   ...       ...   \n",
       "449784  [0.94011855, 0.14096266, 0.98932415, 0.9844897...         0   \n",
       "449785  [0.93586564, 0.14096743, 0.9892238, 0.98398304...         0   \n",
       "449786  [0.92942595, 0.1409559, 0.9890476, 0.98311985,...         0   \n",
       "449787  [0.9255427, 0.14109313, 0.9888827, 0.98237425,...         0   \n",
       "449788  [0.92524135, 0.14110829, 0.98887056, 0.9822847...         0   \n",
       "\n",
       "       Variables_Anómalas  \n",
       "0                      []  \n",
       "1                      []  \n",
       "2                      []  \n",
       "3                      []  \n",
       "4                      []  \n",
       "...                   ...  \n",
       "449784                 []  \n",
       "449785                 []  \n",
       "449786                 []  \n",
       "449787                 []  \n",
       "449788                 []  \n",
       "\n",
       "[449789 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_0</th>\n",
       "      <th>Pred_1</th>\n",
       "      <th>Pred_2</th>\n",
       "      <th>Pred_3</th>\n",
       "      <th>Pred_4</th>\n",
       "      <th>Pred_5</th>\n",
       "      <th>Pred_6</th>\n",
       "      <th>Pred_7</th>\n",
       "      <th>Pred_8</th>\n",
       "      <th>Pred_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Pred_34</th>\n",
       "      <th>Pred_35</th>\n",
       "      <th>Pred_36</th>\n",
       "      <th>Pred_37</th>\n",
       "      <th>Pred_38</th>\n",
       "      <th>Pred_39</th>\n",
       "      <th>Pred_40</th>\n",
       "      <th>Pred_41</th>\n",
       "      <th>Anomalía</th>\n",
       "      <th>Variables_Anómalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968105</td>\n",
       "      <td>0.152933</td>\n",
       "      <td>0.988762</td>\n",
       "      <td>0.970913</td>\n",
       "      <td>0.912299</td>\n",
       "      <td>0.972364</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>0.083441</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.003306</td>\n",
       "      <td>1.018119</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>1.014134</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.972553</td>\n",
       "      <td>0.153398</td>\n",
       "      <td>0.988402</td>\n",
       "      <td>0.972033</td>\n",
       "      <td>0.911338</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.057249</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.004157</td>\n",
       "      <td>1.019529</td>\n",
       "      <td>-0.006162</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>1.000234</td>\n",
       "      <td>1.014375</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.979833</td>\n",
       "      <td>0.153360</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.973167</td>\n",
       "      <td>0.909154</td>\n",
       "      <td>0.963864</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>0.081592</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.005081</td>\n",
       "      <td>1.021845</td>\n",
       "      <td>-0.006085</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>1.001619</td>\n",
       "      <td>1.014978</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985984</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.988913</td>\n",
       "      <td>0.973971</td>\n",
       "      <td>0.906299</td>\n",
       "      <td>0.960823</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.058642</td>\n",
       "      <td>0.080528</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.005688</td>\n",
       "      <td>1.023725</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>1.002506</td>\n",
       "      <td>1.015422</td>\n",
       "      <td>-0.001354</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990039</td>\n",
       "      <td>0.153039</td>\n",
       "      <td>0.989093</td>\n",
       "      <td>0.974487</td>\n",
       "      <td>0.904380</td>\n",
       "      <td>0.959144</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.059077</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.006099</td>\n",
       "      <td>1.025036</td>\n",
       "      <td>-0.006338</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>1.002998</td>\n",
       "      <td>1.015665</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449784</th>\n",
       "      <td>0.940119</td>\n",
       "      <td>0.140963</td>\n",
       "      <td>0.989324</td>\n",
       "      <td>0.984490</td>\n",
       "      <td>0.850793</td>\n",
       "      <td>0.846654</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.056686</td>\n",
       "      <td>0.077648</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001232</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>1.023403</td>\n",
       "      <td>-0.008066</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>1.010761</td>\n",
       "      <td>1.013722</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449785</th>\n",
       "      <td>0.935866</td>\n",
       "      <td>0.140967</td>\n",
       "      <td>0.989224</td>\n",
       "      <td>0.983983</td>\n",
       "      <td>0.850727</td>\n",
       "      <td>0.846742</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.077811</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>1.022474</td>\n",
       "      <td>-0.008085</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>1.010520</td>\n",
       "      <td>1.013614</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449786</th>\n",
       "      <td>0.929426</td>\n",
       "      <td>0.140956</td>\n",
       "      <td>0.989048</td>\n",
       "      <td>0.983120</td>\n",
       "      <td>0.850716</td>\n",
       "      <td>0.846767</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.078048</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>1.021053</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>1.010051</td>\n",
       "      <td>1.013429</td>\n",
       "      <td>-0.002060</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449787</th>\n",
       "      <td>0.925543</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>0.988883</td>\n",
       "      <td>0.982374</td>\n",
       "      <td>0.851560</td>\n",
       "      <td>0.847632</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.078242</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>1.020209</td>\n",
       "      <td>-0.008161</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>1.009548</td>\n",
       "      <td>1.013280</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449788</th>\n",
       "      <td>0.925241</td>\n",
       "      <td>0.141108</td>\n",
       "      <td>0.988871</td>\n",
       "      <td>0.982285</td>\n",
       "      <td>0.851501</td>\n",
       "      <td>0.847557</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.056649</td>\n",
       "      <td>0.078275</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>1.020200</td>\n",
       "      <td>-0.008195</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>1.009446</td>\n",
       "      <td>1.013269</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449789 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pred_0    Pred_1    Pred_2    Pred_3    Pred_4    Pred_5    Pred_6  \\\n",
       "0       0.968105  0.152933  0.988762  0.970913  0.912299  0.972364 -0.000330   \n",
       "1       0.972553  0.153398  0.988402  0.972033  0.911338  0.967407 -0.000158   \n",
       "2       0.979833  0.153360  0.988617  0.973167  0.909154  0.963864  0.000044   \n",
       "3       0.985984  0.153153  0.988913  0.973971  0.906299  0.960823  0.000242   \n",
       "4       0.990039  0.153039  0.989093  0.974487  0.904380  0.959144  0.000376   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "449784  0.940119  0.140963  0.989324  0.984490  0.850793  0.846654  0.000798   \n",
       "449785  0.935866  0.140967  0.989224  0.983983  0.850727  0.846742  0.000719   \n",
       "449786  0.929426  0.140956  0.989048  0.983120  0.850716  0.846767  0.000596   \n",
       "449787  0.925543  0.141093  0.988883  0.982374  0.851560  0.847632  0.000514   \n",
       "449788  0.925241  0.141108  0.988871  0.982285  0.851501  0.847557  0.000507   \n",
       "\n",
       "          Pred_7    Pred_8    Pred_9  ...   Pred_34   Pred_35   Pred_36  \\\n",
       "0       0.056787  0.083441  0.004364  ...  0.000060 -0.003306  1.018119   \n",
       "1       0.057249  0.082886  0.004337  ... -0.000096 -0.004157  1.019529   \n",
       "2       0.058042  0.081592  0.004409  ... -0.000276 -0.005081  1.021845   \n",
       "3       0.058642  0.080528  0.004594  ... -0.000397 -0.005688  1.023725   \n",
       "4       0.059077  0.079820  0.004723  ... -0.000475 -0.006099  1.025036   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "449784  0.056686  0.077648  0.003995  ... -0.001232 -0.001631  1.023403   \n",
       "449785  0.056651  0.077811  0.004053  ... -0.001223 -0.001266  1.022474   \n",
       "449786  0.056632  0.078048  0.004147  ... -0.001191 -0.000731  1.021053   \n",
       "449787  0.056634  0.078242  0.004218  ... -0.001151 -0.000450  1.020209   \n",
       "449788  0.056649  0.078275  0.004236  ... -0.001139 -0.000423  1.020200   \n",
       "\n",
       "         Pred_37   Pred_38   Pred_39   Pred_40   Pred_41  Anomalía  \\\n",
       "0      -0.006454  0.013934  0.998761  1.014134 -0.001079         0   \n",
       "1      -0.006162  0.012472  1.000234  1.014375 -0.001196         0   \n",
       "2      -0.006085  0.011651  1.001619  1.014978 -0.001294         0   \n",
       "3      -0.006218  0.011210  1.002506  1.015422 -0.001354         0   \n",
       "4      -0.006338  0.010974  1.002998  1.015665 -0.001385         0   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "449784 -0.008066 -0.000639  1.010761  1.013722 -0.002061         0   \n",
       "449785 -0.008085 -0.000494  1.010520  1.013614 -0.002059         0   \n",
       "449786 -0.008116 -0.000204  1.010051  1.013429 -0.002060         0   \n",
       "449787 -0.008161  0.000156  1.009548  1.013280 -0.002058         0   \n",
       "449788 -0.008195  0.000246  1.009446  1.013269 -0.002058         0   \n",
       "\n",
       "        Variables_Anómalas  \n",
       "0                       []  \n",
       "1                       []  \n",
       "2                       []  \n",
       "3                       []  \n",
       "4                       []  \n",
       "...                    ...  \n",
       "449784                  []  \n",
       "449785                  []  \n",
       "449786                  []  \n",
       "449787                  []  \n",
       "449788                  []  \n",
       "\n",
       "[449789 rows x 44 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(df_resultados[\"Predicción\"].to_list(), columns=[f\"Pred_{i}\" for i in range(len(df_resultados[\"Predicción\"][0]))])\n",
    "\n",
    "# Concatenar con el DataFrame original\n",
    "salida_modelo = pd.concat([ pred_df, df_resultados[[\"Anomalía\", \"Variables_Anómalas\"]]], axis=1)\n",
    "salida_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT101</th>\n",
       "      <th>LIT101</th>\n",
       "      <th>FIT201</th>\n",
       "      <th>DPIT301</th>\n",
       "      <th>LIT301</th>\n",
       "      <th>LIT401</th>\n",
       "      <th>FIT601</th>\n",
       "      <th>std_FIT101_120</th>\n",
       "      <th>std_LIT101_120</th>\n",
       "      <th>std_FIT201_120</th>\n",
       "      <th>...</th>\n",
       "      <th>MV303_2</th>\n",
       "      <th>MV304_0</th>\n",
       "      <th>MV304_1</th>\n",
       "      <th>MV304_2</th>\n",
       "      <th>P302_1</th>\n",
       "      <th>P302_2</th>\n",
       "      <th>P602_1</th>\n",
       "      <th>P602_2</th>\n",
       "      <th>Anomalía</th>\n",
       "      <th>Variables_Anómalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968105</td>\n",
       "      <td>0.152933</td>\n",
       "      <td>0.988762</td>\n",
       "      <td>0.970913</td>\n",
       "      <td>0.912299</td>\n",
       "      <td>0.972364</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>0.083441</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.003306</td>\n",
       "      <td>1.018119</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>1.014134</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.972553</td>\n",
       "      <td>0.153398</td>\n",
       "      <td>0.988402</td>\n",
       "      <td>0.972033</td>\n",
       "      <td>0.911338</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.057249</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.004157</td>\n",
       "      <td>1.019529</td>\n",
       "      <td>-0.006162</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>1.000234</td>\n",
       "      <td>1.014375</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.979833</td>\n",
       "      <td>0.153360</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.973167</td>\n",
       "      <td>0.909154</td>\n",
       "      <td>0.963864</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>0.081592</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.005081</td>\n",
       "      <td>1.021845</td>\n",
       "      <td>-0.006085</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>1.001619</td>\n",
       "      <td>1.014978</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985984</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.988913</td>\n",
       "      <td>0.973971</td>\n",
       "      <td>0.906299</td>\n",
       "      <td>0.960823</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.058642</td>\n",
       "      <td>0.080528</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.005688</td>\n",
       "      <td>1.023725</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>1.002506</td>\n",
       "      <td>1.015422</td>\n",
       "      <td>-0.001354</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990039</td>\n",
       "      <td>0.153039</td>\n",
       "      <td>0.989093</td>\n",
       "      <td>0.974487</td>\n",
       "      <td>0.904380</td>\n",
       "      <td>0.959144</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.059077</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.006099</td>\n",
       "      <td>1.025036</td>\n",
       "      <td>-0.006338</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>1.002998</td>\n",
       "      <td>1.015665</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449784</th>\n",
       "      <td>0.940119</td>\n",
       "      <td>0.140963</td>\n",
       "      <td>0.989324</td>\n",
       "      <td>0.984490</td>\n",
       "      <td>0.850793</td>\n",
       "      <td>0.846654</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.056686</td>\n",
       "      <td>0.077648</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001232</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>1.023403</td>\n",
       "      <td>-0.008066</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>1.010761</td>\n",
       "      <td>1.013722</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449785</th>\n",
       "      <td>0.935866</td>\n",
       "      <td>0.140967</td>\n",
       "      <td>0.989224</td>\n",
       "      <td>0.983983</td>\n",
       "      <td>0.850727</td>\n",
       "      <td>0.846742</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.077811</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>1.022474</td>\n",
       "      <td>-0.008085</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>1.010520</td>\n",
       "      <td>1.013614</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449786</th>\n",
       "      <td>0.929426</td>\n",
       "      <td>0.140956</td>\n",
       "      <td>0.989048</td>\n",
       "      <td>0.983120</td>\n",
       "      <td>0.850716</td>\n",
       "      <td>0.846767</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.078048</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>1.021053</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>1.010051</td>\n",
       "      <td>1.013429</td>\n",
       "      <td>-0.002060</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449787</th>\n",
       "      <td>0.925543</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>0.988883</td>\n",
       "      <td>0.982374</td>\n",
       "      <td>0.851560</td>\n",
       "      <td>0.847632</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.078242</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>1.020209</td>\n",
       "      <td>-0.008161</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>1.009548</td>\n",
       "      <td>1.013280</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449788</th>\n",
       "      <td>0.925241</td>\n",
       "      <td>0.141108</td>\n",
       "      <td>0.988871</td>\n",
       "      <td>0.982285</td>\n",
       "      <td>0.851501</td>\n",
       "      <td>0.847557</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.056649</td>\n",
       "      <td>0.078275</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>1.020200</td>\n",
       "      <td>-0.008195</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>1.009446</td>\n",
       "      <td>1.013269</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449789 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FIT101    LIT101    FIT201   DPIT301    LIT301    LIT401    FIT601  \\\n",
       "0       0.968105  0.152933  0.988762  0.970913  0.912299  0.972364 -0.000330   \n",
       "1       0.972553  0.153398  0.988402  0.972033  0.911338  0.967407 -0.000158   \n",
       "2       0.979833  0.153360  0.988617  0.973167  0.909154  0.963864  0.000044   \n",
       "3       0.985984  0.153153  0.988913  0.973971  0.906299  0.960823  0.000242   \n",
       "4       0.990039  0.153039  0.989093  0.974487  0.904380  0.959144  0.000376   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "449784  0.940119  0.140963  0.989324  0.984490  0.850793  0.846654  0.000798   \n",
       "449785  0.935866  0.140967  0.989224  0.983983  0.850727  0.846742  0.000719   \n",
       "449786  0.929426  0.140956  0.989048  0.983120  0.850716  0.846767  0.000596   \n",
       "449787  0.925543  0.141093  0.988883  0.982374  0.851560  0.847632  0.000514   \n",
       "449788  0.925241  0.141108  0.988871  0.982285  0.851501  0.847557  0.000507   \n",
       "\n",
       "        std_FIT101_120  std_LIT101_120  std_FIT201_120  ...   MV303_2  \\\n",
       "0             0.056787        0.083441        0.004364  ...  0.000060   \n",
       "1             0.057249        0.082886        0.004337  ... -0.000096   \n",
       "2             0.058042        0.081592        0.004409  ... -0.000276   \n",
       "3             0.058642        0.080528        0.004594  ... -0.000397   \n",
       "4             0.059077        0.079820        0.004723  ... -0.000475   \n",
       "...                ...             ...             ...  ...       ...   \n",
       "449784        0.056686        0.077648        0.003995  ... -0.001232   \n",
       "449785        0.056651        0.077811        0.004053  ... -0.001223   \n",
       "449786        0.056632        0.078048        0.004147  ... -0.001191   \n",
       "449787        0.056634        0.078242        0.004218  ... -0.001151   \n",
       "449788        0.056649        0.078275        0.004236  ... -0.001139   \n",
       "\n",
       "         MV304_0   MV304_1   MV304_2    P302_1    P302_2    P602_1    P602_2  \\\n",
       "0      -0.003306  1.018119 -0.006454  0.013934  0.998761  1.014134 -0.001079   \n",
       "1      -0.004157  1.019529 -0.006162  0.012472  1.000234  1.014375 -0.001196   \n",
       "2      -0.005081  1.021845 -0.006085  0.011651  1.001619  1.014978 -0.001294   \n",
       "3      -0.005688  1.023725 -0.006218  0.011210  1.002506  1.015422 -0.001354   \n",
       "4      -0.006099  1.025036 -0.006338  0.010974  1.002998  1.015665 -0.001385   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "449784 -0.001631  1.023403 -0.008066 -0.000639  1.010761  1.013722 -0.002061   \n",
       "449785 -0.001266  1.022474 -0.008085 -0.000494  1.010520  1.013614 -0.002059   \n",
       "449786 -0.000731  1.021053 -0.008116 -0.000204  1.010051  1.013429 -0.002060   \n",
       "449787 -0.000450  1.020209 -0.008161  0.000156  1.009548  1.013280 -0.002058   \n",
       "449788 -0.000423  1.020200 -0.008195  0.000246  1.009446  1.013269 -0.002058   \n",
       "\n",
       "        Anomalía  Variables_Anómalas  \n",
       "0              0                  []  \n",
       "1              0                  []  \n",
       "2              0                  []  \n",
       "3              0                  []  \n",
       "4              0                  []  \n",
       "...          ...                 ...  \n",
       "449784         0                  []  \n",
       "449785         0                  []  \n",
       "449786         0                  []  \n",
       "449787         0                  []  \n",
       "449788         0                  []  \n",
       "\n",
       "[449789 rows x 44 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un diccionario de mapeo entre los nombres Pred_X y los nombres originales\n",
    "mapeo_nombres = {f\"Pred_{i}\": X_train.columns[i] for i in range(42)}\n",
    "\n",
    "# Renombrar las columnas usando el diccionario\n",
    "salida_modelo.rename(columns=mapeo_nombres, inplace=True)\n",
    "salida_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_variables = salida_modelo.columns[:-2].tolist()\n",
    "\n",
    "# Convertir los índices en nombres\n",
    "salida_modelo[\"Variables_Anómalas\"] = salida_modelo[\"Variables_Anómalas\"].apply(lambda indices: [nombres_variables[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIT101</th>\n",
       "      <th>LIT101</th>\n",
       "      <th>FIT201</th>\n",
       "      <th>DPIT301</th>\n",
       "      <th>LIT301</th>\n",
       "      <th>LIT401</th>\n",
       "      <th>FIT601</th>\n",
       "      <th>std_FIT101_120</th>\n",
       "      <th>std_LIT101_120</th>\n",
       "      <th>std_FIT201_120</th>\n",
       "      <th>...</th>\n",
       "      <th>MV303_2</th>\n",
       "      <th>MV304_0</th>\n",
       "      <th>MV304_1</th>\n",
       "      <th>MV304_2</th>\n",
       "      <th>P302_1</th>\n",
       "      <th>P302_2</th>\n",
       "      <th>P602_1</th>\n",
       "      <th>P602_2</th>\n",
       "      <th>Anomalía</th>\n",
       "      <th>Variables_Anómalas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968105</td>\n",
       "      <td>0.152933</td>\n",
       "      <td>0.988762</td>\n",
       "      <td>0.970913</td>\n",
       "      <td>0.912299</td>\n",
       "      <td>0.972364</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.056787</td>\n",
       "      <td>0.083441</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.003306</td>\n",
       "      <td>1.018119</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>1.014134</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.972553</td>\n",
       "      <td>0.153398</td>\n",
       "      <td>0.988402</td>\n",
       "      <td>0.972033</td>\n",
       "      <td>0.911338</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.057249</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.004337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>-0.004157</td>\n",
       "      <td>1.019529</td>\n",
       "      <td>-0.006162</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>1.000234</td>\n",
       "      <td>1.014375</td>\n",
       "      <td>-0.001196</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.979833</td>\n",
       "      <td>0.153360</td>\n",
       "      <td>0.988617</td>\n",
       "      <td>0.973167</td>\n",
       "      <td>0.909154</td>\n",
       "      <td>0.963864</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>0.081592</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.005081</td>\n",
       "      <td>1.021845</td>\n",
       "      <td>-0.006085</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>1.001619</td>\n",
       "      <td>1.014978</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.985984</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.988913</td>\n",
       "      <td>0.973971</td>\n",
       "      <td>0.906299</td>\n",
       "      <td>0.960823</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.058642</td>\n",
       "      <td>0.080528</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>-0.005688</td>\n",
       "      <td>1.023725</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>1.002506</td>\n",
       "      <td>1.015422</td>\n",
       "      <td>-0.001354</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990039</td>\n",
       "      <td>0.153039</td>\n",
       "      <td>0.989093</td>\n",
       "      <td>0.974487</td>\n",
       "      <td>0.904380</td>\n",
       "      <td>0.959144</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.059077</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.006099</td>\n",
       "      <td>1.025036</td>\n",
       "      <td>-0.006338</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>1.002998</td>\n",
       "      <td>1.015665</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449784</th>\n",
       "      <td>0.940119</td>\n",
       "      <td>0.140963</td>\n",
       "      <td>0.989324</td>\n",
       "      <td>0.984490</td>\n",
       "      <td>0.850793</td>\n",
       "      <td>0.846654</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.056686</td>\n",
       "      <td>0.077648</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001232</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>1.023403</td>\n",
       "      <td>-0.008066</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>1.010761</td>\n",
       "      <td>1.013722</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449785</th>\n",
       "      <td>0.935866</td>\n",
       "      <td>0.140967</td>\n",
       "      <td>0.989224</td>\n",
       "      <td>0.983983</td>\n",
       "      <td>0.850727</td>\n",
       "      <td>0.846742</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.077811</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>-0.001266</td>\n",
       "      <td>1.022474</td>\n",
       "      <td>-0.008085</td>\n",
       "      <td>-0.000494</td>\n",
       "      <td>1.010520</td>\n",
       "      <td>1.013614</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449786</th>\n",
       "      <td>0.929426</td>\n",
       "      <td>0.140956</td>\n",
       "      <td>0.989048</td>\n",
       "      <td>0.983120</td>\n",
       "      <td>0.850716</td>\n",
       "      <td>0.846767</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.078048</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001191</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>1.021053</td>\n",
       "      <td>-0.008116</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>1.010051</td>\n",
       "      <td>1.013429</td>\n",
       "      <td>-0.002060</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449787</th>\n",
       "      <td>0.925543</td>\n",
       "      <td>0.141093</td>\n",
       "      <td>0.988883</td>\n",
       "      <td>0.982374</td>\n",
       "      <td>0.851560</td>\n",
       "      <td>0.847632</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.078242</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.000450</td>\n",
       "      <td>1.020209</td>\n",
       "      <td>-0.008161</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>1.009548</td>\n",
       "      <td>1.013280</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449788</th>\n",
       "      <td>0.925241</td>\n",
       "      <td>0.141108</td>\n",
       "      <td>0.988871</td>\n",
       "      <td>0.982285</td>\n",
       "      <td>0.851501</td>\n",
       "      <td>0.847557</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.056649</td>\n",
       "      <td>0.078275</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>1.020200</td>\n",
       "      <td>-0.008195</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>1.009446</td>\n",
       "      <td>1.013269</td>\n",
       "      <td>-0.002058</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449789 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          FIT101    LIT101    FIT201   DPIT301    LIT301    LIT401    FIT601  \\\n",
       "0       0.968105  0.152933  0.988762  0.970913  0.912299  0.972364 -0.000330   \n",
       "1       0.972553  0.153398  0.988402  0.972033  0.911338  0.967407 -0.000158   \n",
       "2       0.979833  0.153360  0.988617  0.973167  0.909154  0.963864  0.000044   \n",
       "3       0.985984  0.153153  0.988913  0.973971  0.906299  0.960823  0.000242   \n",
       "4       0.990039  0.153039  0.989093  0.974487  0.904380  0.959144  0.000376   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "449784  0.940119  0.140963  0.989324  0.984490  0.850793  0.846654  0.000798   \n",
       "449785  0.935866  0.140967  0.989224  0.983983  0.850727  0.846742  0.000719   \n",
       "449786  0.929426  0.140956  0.989048  0.983120  0.850716  0.846767  0.000596   \n",
       "449787  0.925543  0.141093  0.988883  0.982374  0.851560  0.847632  0.000514   \n",
       "449788  0.925241  0.141108  0.988871  0.982285  0.851501  0.847557  0.000507   \n",
       "\n",
       "        std_FIT101_120  std_LIT101_120  std_FIT201_120  ...   MV303_2  \\\n",
       "0             0.056787        0.083441        0.004364  ...  0.000060   \n",
       "1             0.057249        0.082886        0.004337  ... -0.000096   \n",
       "2             0.058042        0.081592        0.004409  ... -0.000276   \n",
       "3             0.058642        0.080528        0.004594  ... -0.000397   \n",
       "4             0.059077        0.079820        0.004723  ... -0.000475   \n",
       "...                ...             ...             ...  ...       ...   \n",
       "449784        0.056686        0.077648        0.003995  ... -0.001232   \n",
       "449785        0.056651        0.077811        0.004053  ... -0.001223   \n",
       "449786        0.056632        0.078048        0.004147  ... -0.001191   \n",
       "449787        0.056634        0.078242        0.004218  ... -0.001151   \n",
       "449788        0.056649        0.078275        0.004236  ... -0.001139   \n",
       "\n",
       "         MV304_0   MV304_1   MV304_2    P302_1    P302_2    P602_1    P602_2  \\\n",
       "0      -0.003306  1.018119 -0.006454  0.013934  0.998761  1.014134 -0.001079   \n",
       "1      -0.004157  1.019529 -0.006162  0.012472  1.000234  1.014375 -0.001196   \n",
       "2      -0.005081  1.021845 -0.006085  0.011651  1.001619  1.014978 -0.001294   \n",
       "3      -0.005688  1.023725 -0.006218  0.011210  1.002506  1.015422 -0.001354   \n",
       "4      -0.006099  1.025036 -0.006338  0.010974  1.002998  1.015665 -0.001385   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "449784 -0.001631  1.023403 -0.008066 -0.000639  1.010761  1.013722 -0.002061   \n",
       "449785 -0.001266  1.022474 -0.008085 -0.000494  1.010520  1.013614 -0.002059   \n",
       "449786 -0.000731  1.021053 -0.008116 -0.000204  1.010051  1.013429 -0.002060   \n",
       "449787 -0.000450  1.020209 -0.008161  0.000156  1.009548  1.013280 -0.002058   \n",
       "449788 -0.000423  1.020200 -0.008195  0.000246  1.009446  1.013269 -0.002058   \n",
       "\n",
       "        Anomalía  Variables_Anómalas  \n",
       "0              0                  []  \n",
       "1              0                  []  \n",
       "2              0                  []  \n",
       "3              0                  []  \n",
       "4              0                  []  \n",
       "...          ...                 ...  \n",
       "449784         0                  []  \n",
       "449785         0                  []  \n",
       "449786         0                  []  \n",
       "449787         0                  []  \n",
       "449788         0                  []  \n",
       "\n",
       "[449789 rows x 44 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salida_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida_modelo.to_csv(\"salida_modelo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6573266,
     "sourceId": 10616915,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6681610,
     "sourceId": 10770534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6694428,
     "sourceId": 10787557,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6694454,
     "sourceId": 10787593,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6704694,
     "sourceId": 10802338,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6705374,
     "sourceId": 10803309,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6705389,
     "sourceId": 10803331,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6705593,
     "sourceId": 10803597,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30888,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
